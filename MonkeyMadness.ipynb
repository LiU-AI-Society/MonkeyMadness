{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonkeyMadness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by enabling the GPU: (Deep learning requires many computations and the training time will be much faster if we do it on the GPU)\n",
    "1. Go to \"RunTime\" menu and select \n",
    "2. \"Change runtime type.\" A dialog box will appear where you can choose the runtime type and hardware accelerator.\n",
    "3. Select \"GPU\" as the hardware accelerator and click \"Save.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment and enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/LiU-AI-Society/MonkeyMadness.git\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "%cd {HOME}/Classification-Game\n",
    "\n",
    "%pip install req_colab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "In this task, you will use supervised learning to classify images of monkeys. Supervised learning is one of the three main areas of machine learning.  \n",
    "\n",
    "Supervised learning is like teaching the computer to recognize monkeys by using flashcards.  \n",
    "\n",
    "1. **Labeled Data**: You show the computer images of monkeys and tell it which species each one is (e.g., \"This is a chimpanzee\").  \n",
    "2. **Training**: The computer studies these examples to learn patterns.  \n",
    "3. **Comparison**: It makes a prediction of the image and then compares it to the labeled data. It will be penalized based on how \"wrong\" it is.  \n",
    "4. **Prediction**: Once trained, it can look at a new image and guess the species of the monkey based on what it learned.  \n",
    "\n",
    "It’s called “supervised” because the model learns under guidance (the labeled data).  \n",
    "\n",
    "## There are some main ingredients:\n",
    "\n",
    "1. **Data**  \n",
    "    - The data has to be labeled, i.e., someone has to manually note down what monkey is present in the image.  \n",
    "\n",
    "2. **Model**  \n",
    "    - It is the model that makes predictions. It does so by looking at the input and making a guess about what monkey is present. The model consists of mathematical operations and weights (these are adjustable). In this task we will use so called Convolutional Neural Network which are very good at handling image data.\n",
    "\n",
    "3. **Loss Function**  \n",
    "    - Somehow we need to tell the computer how wrong the guess is.  \n",
    "    - The model will output probabilities for each monkey class. Let’s say it sees an image of an orangutan, then it could perhaps output the following:  \n",
    "\n",
    "        ```markdown\n",
    "        Chimpanzee: 0.70 (70%)  \n",
    "        Orangutan: 0.20 (20%)  \n",
    "        Other monkeys: 0.10 (10%)  \n",
    "        ```\n",
    "\n",
    "        The correct label is orangutan (100% probability), but the model guessed only 20% for this class.\n",
    "\n",
    "        Using a loss function like Cross-Entropy Loss, the score is calculated to show how wrong the prediction is\n",
    "        The closer the prediction is to 1 (100%), the smaller the loss. \n",
    "        The goal is to adjust the model to make higher confidence predictions for the correct class.\n",
    "4. **Optimizer**\n",
    "    - Somehow we need to adjust the model to perform better. This is done by calculating, based on the loss how the model should be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Goal of the hackathon?\n",
    "\n",
    "You will be provided with a data set. This will be divided into a training set and validation set. The model will learn on the training set and you will use the validation set to give an estimate on how it would perform on unseen data. The goal is to tune and experiment with different parameters and architectures to get a model that performs as well as possible on a test set (which you do not have acces to). In the end you will send your best model to us and we can test it (live testing). Due to time and to mimic real life scenario we have limited your resources and thus you will have to think which improvements you will implement. The improvements are available in the shop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a convolution? \n",
    "\n",
    "\n",
    "The convolution is like sliding a small \"window\" (called a kernel or filter) over an image to look for patterns.\n",
    "\n",
    "Here's how it works step-by-step:\n",
    "\n",
    "Kernel: Think of this as a small grid of numbers (e.g., 3x3).\n",
    "Slide and Multiply: Place the kernel on part of the image. Multiply the numbers in the kernel with the corresponding numbers in the image under it.\n",
    "Sum Up: Add the results of the multiplication together. This gives one number for that position.\n",
    "Move the Kernel: Slide the kernel to the next part of the image and repeat.\n",
    "The result is a new image (called a feature map) that highlights certain patterns like edges or textures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from convolution import perform_convolution, plot_images\n",
    "\n",
    "# Load the image\n",
    "image_path = 'Monkey/training/training/n7/n7023.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Transform the image to a tensor\n",
    "transform = transforms.ToTensor()\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Define vertical and horizontal line detection kernels\n",
    "vertical_line_kernel = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                     [-1.0, 0.0, 1.0],\n",
    "                                     [-1.0, 0.0, 1.0]], dtype=torch.float32)\n",
    "\n",
    "horizontal_line_kernel = torch.tensor([[-1.0, -1.0, -1.0],\n",
    "                                       [0.0,  0.0,  0.0],\n",
    "                                       [1.0,  1.0,  1.0]], dtype=torch.float32)\n",
    "\n",
    "# Perform the convolutions\n",
    "horizontal_lines_image = perform_convolution(horizontal_line_kernel, image)\n",
    "vertical_line_image = perform_convolution(vertical_line_kernel, image)\n",
    "\n",
    "# plot the result\n",
    "\n",
    "plot_images(image, vertical_line_image, horizontal_lines_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset. \n",
    "### You will use a monkey dataset consisting of ca 1000 images of monkey. The goal is to be able to classify them.\n",
    "### There are ten monkey classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "from Dataset import CustomImageDataset, MonkeyImageDataset\n",
    "\n",
    "NUM_OF_CLASSES = 10\n",
    "IMAGE_SIZE = (64, 64) # DO NOT ALTER THIS PARAMETER\n",
    "\n",
    "DATA_PERCENTAGE = 0.7\n",
    "transform = transforms.Compose([\n",
    "    #Randomly flip the images vertically\n",
    "    #transforms.RandomVerticalFlip(p=0.2),  # Randomly flip the image vertically with 20% probability\n",
    "    #transforms.RandomHorizontalFlip(p=0.2),  # Randomly flip the image horizontally with 20% probability\n",
    "    #transforms.RandomRotation(degrees=15),  # Rotate the image randomly within a 15-degree range\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change brightness, contrast, etc.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMAGE_SIZE[0], IMAGE_SIZE[1])), \n",
    "\n",
    "    #for imagenet\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = MonkeyImageDataset('Monkey/training/training', transform, NUM_OF_CLASSES, data_percentage = DATA_PERCENTAGE )\n",
    "dataset.visualize(5)\n",
    "#dataset.visualize_all_classes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define our model! \n",
    "### Lets build a Convolutional Neural Network (CNN). It uses convolutions (one can think of it as filters) to learn the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Average and Max pooling\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        # Concatenate pooled outputs\n",
    "        combined = torch.cat([avg_out, max_out], dim=1)\n",
    "        # Convolve and apply sigmoid\n",
    "        attention_map = self.sigmoid(self.conv(combined))\n",
    "        return x * attention_map\n",
    "\n",
    "\n",
    "class MonkeyNET(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_size=(500, 500)):\n",
    "        super(MonkeyNET, self).__init__()\n",
    "        \n",
    "        # First convolutional layer: 3 input channels (RGB), 32 output channels, kernel size 5, padding 2 to preserve size\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        #self.attention = SpatialAttention()\n",
    "        # Calculate the size of the fully connected layer dynamically\n",
    "        self.fc_input_size = self._get_fc_input_size(input_size)\n",
    "        self.fc = nn.Linear(self.fc_input_size, num_classes)  # Adjusted for the final size after pooling\n",
    "        \n",
    "        # Prediction layer\n",
    "        #self.prediction = nn.Linear(16, num_classes)\n",
    "        \n",
    "    def _get_fc_input_size(self, input_size):\n",
    "        x = torch.zeros(1, 3, *input_size)  # Create a dummy input tensor\n",
    "        #x = self.attention(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=(4, 4), stride=4)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=(8, 8), stride=8)\n",
    "\n",
    "\n",
    "        return x.numel()  # Total number of elements after conv layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv -> ReLU -> Max Pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        #x = self.attention(x)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=(4, 4), stride=4)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=(8, 8), stride=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Flatten the tensor for fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Output: (batch_size, 128 * 16 * 16) for 500x500 input\n",
    "\n",
    "        # Fully connected layer -> ReLU\n",
    "        x = F.relu(self.fc(x))\n",
    "\n",
    "        # Output layer (no activation, to be combined with a loss function later)\n",
    "        #x = self.prediction(x)\n",
    "        # Optionally remove Softmax from here\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the summary of the model in a compact way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
      "            Conv2d-2           [-1, 32, 16, 16]          18,464\n",
      "            Linear-3                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 21,546\n",
      "Trainable params: 21,546\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 2.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 2.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import torchlens as tl\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "\n",
    "model = MonkeyNET(num_classes=NUM_OF_CLASSES, input_size=IMAGE_SIZE)\n",
    "\n",
    "summary(model, (3, IMAGE_SIZE[0], IMAGE_SIZE[1]))\n",
    "\n",
    "# ---- Uncomment the lines below to get a visual of the model ------\n",
    "#import torchlens as tl\n",
    "#import graphviz\n",
    "#graphviz.set_jupyter_format('png')\n",
    "#x = torch.rand(1, 3, IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
    "#model_hist = tl.log_forward_pass(model,x, vis_opt='unrolled')\n",
    "#model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple GPU)\n",
      "15:54\n",
      "Starting training on device: mps\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Move models to gpu and train \u001b[39;00m\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 72\u001b[0m model, t_loss, t_acc, v_loss, v_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTART_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m t_loss\n\u001b[1;32m     76\u001b[0m model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m t_acc\n",
      "File \u001b[0;32m~/Desktop/MonkeyMadness/train.py:37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, valloader, optimizer, objective, device, start_epoch, num_epochs, model_name, unique_id, scheduler, teacher_model)\u001b[0m\n\u001b[1;32m     34\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     38\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vis_rec/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vis_rec/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vis_rec/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vis_rec/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vis_rec/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/Desktop/MonkeyMadness/Dataset.py:187\u001b[0m, in \u001b[0;36mMonkeyImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    184\u001b[0m target[label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Apply transformations if provided\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vis_rec/lib/python3.9/site-packages/PIL/Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/vis_rec/lib/python3.9/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import train, training_info\n",
    "from test_model import test\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (GPU)\")\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders for macOS) is available (for Apple Silicon Macs)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple GPU)\")\n",
    "\n",
    "# Fallback to CPU if neither CUDA nor MPS is available\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 30 # how long should the model train\n",
    "MODEL_NAME = \"BaseLie \"\n",
    "\n",
    "current_datetime = datetime.datetime.now()\n",
    "ID = f\"{current_datetime.hour:02}:{current_datetime.minute:02}\"\n",
    "print(ID)\n",
    "LR = 0.01 # how \"much\" should the model learn\n",
    "BATCH_SIZE = 8# how many images should the model see before updating\n",
    "\n",
    "model_info = {\n",
    "            'epochs' : EPOCHS,\n",
    "            'batch_size' : BATCH_SIZE,\n",
    "            'lr' : LR,\n",
    "            'ID' : ID,\n",
    "            'model_name' : MODEL_NAME\n",
    "\n",
    "}\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)  # 80% for training\n",
    "val_size = dataset_size - train_size   # 20% for validation\n",
    "if NUM_OF_CLASSES > 2:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion = nn.BCELoss()\n",
    "#criterion = DistillationLoss()\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = MonkeyNET(num_classes=NUM_OF_CLASSES, input_size=IMAGE_SIZE)\n",
    "\n",
    "\n",
    "# Create optimizers for the model. This will try to find the optimal parameters in the model i.e this adjusts the model to improve the loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "\n",
    "# Move models to gpu and train \n",
    "model.to(device)\n",
    "model, t_loss, t_acc, v_loss, v_acc = train(model, train_loader, val_loader, optimizer, criterion, device, start_epoch=START_EPOCH, num_epochs=EPOCHS, model_name=MODEL_NAME, unique_id=ID)\n",
    "\n",
    "\n",
    "model_info['t_loss'] = t_loss\n",
    "model_info['t_acc'] = t_acc\n",
    "\n",
    "model_info['v_loss'] = v_loss\n",
    "model_info['v_acc'] = v_acc\n",
    "\n",
    "training_info(model_info=model_info)\n",
    "\n",
    "# see metrics on the validation set\n",
    "\n",
    "acc = test(model=model, testloader=val_loader, device=device, model_name=MODEL_NAME, unique_id=ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you are happy with the results you can save the model. Give it a good name like your group name and a brief description e.g 'GroupBananasBest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Assuming model1 is defined and trained\n",
    "save_model_name = \"Pretrained_ResNET\"\n",
    "\n",
    "# Define the directory to save the model\n",
    "model_dir = 'saved_models'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a dummy input tensor matching the input shape expected by the model\n",
    "# Here we assume the model expects a 1x3x224x224 image (batch size 1, 3 channels, 224x224 pixels)\n",
    "# You should adjust the shape based on your actual model's input\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_path = f\"{model_dir}/{save_model_name}.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_path, verbose=True)\n",
    "\n",
    "\n",
    "print(f\"Model saved to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See all performed experiments to compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from train import plot_experiments\n",
    "\n",
    "plot_experiments('training_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send your best model to us for us to run it on a test set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vis_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
